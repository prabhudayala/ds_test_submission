{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b10167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder, Binarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('Training/X_train.csv')\n",
    "y = pd.read_csv('Training/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf9bb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C6'] = X['C6'].apply(lambda x: 0 if x==False else 1)\n",
    "X['C8'] = X['C8'].apply(lambda x: 0 if x==False else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1122ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,1:]\n",
    "y = y['Dependent_Variable']\n",
    "X_numerical = X.iloc[:,8:]\n",
    "numerical_columns= list(X_numerical.columns)\n",
    "X_categorical = X.iloc[:,:8]\n",
    "categorical_columns= list(X_categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2041a38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>N4</th>\n",
       "      <th>N5</th>\n",
       "      <th>N6</th>\n",
       "      <th>N7</th>\n",
       "      <th>N8</th>\n",
       "      <th>N9</th>\n",
       "      <th>N10</th>\n",
       "      <th>...</th>\n",
       "      <th>N26</th>\n",
       "      <th>N27</th>\n",
       "      <th>N28</th>\n",
       "      <th>N29</th>\n",
       "      <th>N30</th>\n",
       "      <th>N31</th>\n",
       "      <th>N32</th>\n",
       "      <th>N33</th>\n",
       "      <th>N34</th>\n",
       "      <th>N35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.595</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>113.39</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.05</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.795</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>262.10</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.495</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>50.29</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.595</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>126.52</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.50</td>\n",
       "      <td>206.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.895</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>205.47</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      N1     N2   N3    N4    N5     N6    N7  N8    N9  N10  ...  N26  N27  \\\n",
       "0  23.75    NaN  2.5   NaN   NaN  2.595  10.0   0     0  2.0  ...  NaN  NaN   \n",
       "1  11.05   22.0  3.7  16.0  12.0  3.795  19.0   4    72  0.0  ...  NaN  NaN   \n",
       "2  29.00    NaN  2.4   NaN   NaN  2.495  17.0   2    15  7.0  ...  NaN  NaN   \n",
       "3  17.99    1.0  3.5   4.0   4.0  3.595   6.0   5   190  0.0  ...  NaN  NaN   \n",
       "4  27.50  206.0  3.8  11.0  11.0  3.895  31.0  10  1504  0.0  ...  NaN  NaN   \n",
       "\n",
       "   N28  N29  N30  N31  N32    N33     N34   N35  \n",
       "0  NaN  NaN  NaN  NaN  NaN   58.0  113.39  12.0  \n",
       "1  NaN  NaN  NaN  NaN  NaN  160.0  262.10  17.0  \n",
       "2  NaN  NaN  NaN  NaN  NaN   24.0   50.29  18.0  \n",
       "3  NaN  NaN  NaN  NaN  NaN   70.0  126.52  27.0  \n",
       "4  NaN  NaN  NaN  NaN  NaN  100.0  205.47  21.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C1  C2  C3  C4  C5  C6  C7  C8\n",
       "0   1   0  11  31   0   0   0   1\n",
       "1   1   4   2  66   2   0   1   1\n",
       "2   1   0  19   2   0   0   0   1\n",
       "3   1   1  16  47   1   0   4   1\n",
       "4   1   1  13   1   1   1   6   1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf45e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98686288",
   "metadata": {},
   "source": [
    "### Logistic regression and polynomial degree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3230536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', LogisticRegression())])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ffe3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7705835206700378\n",
      "Test AUC is:  0.7587575880165814\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f1efa",
   "metadata": {},
   "source": [
    "### Logistic regression and polynomial degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee95ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', LogisticRegression())])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d80291d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7942182065955579\n",
      "Test AUC is:  0.7576909650183259\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89de94f",
   "metadata": {},
   "source": [
    "### Logistic regression hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42a65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513c391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=5)]: Done 120 out of 120 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565010694676705\n",
      "{'model__C': 0.01, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "grid = {\n",
    "         'model__solver':['saga', 'lbfgs',],\n",
    "         'model__penalty':['elasticnet', 'l1', 'l2', 'none'],\n",
    "         'model__C':[100, 10, 1.0, 0.1, 0.01],\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator = full_pipeline, param_grid = grid, cv = 3, verbose=10, n_jobs = 5, scoring='roc_auc')\n",
    "grid_search.fit(X_train,y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c162f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', LogisticRegression(C = 0.01, penalty = 'l2', solver = 'lbfgs'))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8282139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7830362525308012\n",
      "Test AUC is:  0.7604561190755026\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68233081",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e80b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', SGDClassifier(loss='log', max_iter=10000))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3af0224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7533345997093897\n",
      "Test AUC is:  0.7428038097380557\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d7525",
   "metadata": {},
   "source": [
    "### Ramdom forest and polynomial degree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ee5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', RandomForestClassifier())])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815f748f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  1.0\n",
      "Test AUC is:  0.7648245355347758\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13b9d9",
   "metadata": {},
   "source": [
    "### Ramdom forest and polynomial degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db7e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', RandomForestClassifier())])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2334287b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  1.0\n",
      "Test AUC is:  0.753052456600184\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316be67f",
   "metadata": {},
   "source": [
    "### Ramdom forest and polynomial degree 1 and class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a78d3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', RandomForestClassifier(class_weight='balanced'))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4330c40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  1.0\n",
      "Test AUC is:  0.7529324354213198\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adf88b",
   "metadata": {},
   "source": [
    "### Random forest hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54d3f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c532beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=5)]: Done  25 out of  30 | elapsed:  6.9min remaining:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7567508710194844\n",
      "{'model__n_estimators': 700, 'model__min_samples_split': 3, 'model__min_samples_leaf': 4, 'model__max_depth': None, 'model__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(5, 15, num = 1)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2,3,4,5,6]\n",
    "min_samples_leaf = [1,2,3,4,5,6]\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'model__n_estimators': n_estimators,\n",
    "               'model__max_depth': max_depth,\n",
    "               'model__min_samples_split': min_samples_split,\n",
    "               'model__min_samples_leaf': min_samples_leaf,\n",
    "               'model__bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "rf_random.fit(X_train,y_train)\n",
    "print(rf_random.best_score_)\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a74bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=5)]: Done   8 out of  15 | elapsed: 16.8min remaining: 14.7min\n",
      "[Parallel(n_jobs=5)]: Done  10 out of  15 | elapsed: 17.2min remaining:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done  12 out of  15 | elapsed: 25.9min remaining:  6.5min\n",
      "[Parallel(n_jobs=5)]: Done  15 out of  15 | elapsed: 29.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578670909234434\n",
      "{'model__n_estimators': 4604, 'model__min_samples_split': 3, 'model__min_samples_leaf': 5, 'model__max_depth': None, 'model__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 5000, num = 100)]\n",
    "max_depth = [int(x) for x in np.linspace(5, 15, num = 1)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2,3,4,5,6]\n",
    "min_samples_leaf = [1,2,3,4,5,6]\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'model__n_estimators': n_estimators,\n",
    "               'model__max_depth': max_depth,\n",
    "               'model__min_samples_split': min_samples_split,\n",
    "               'model__min_samples_leaf': min_samples_leaf,\n",
    "               'model__bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rf_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 5, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "rf_random.fit(X_train,y_train)\n",
    "print(rf_random.best_score_)\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6461f",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0bf7cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 30)),\n",
    "#         ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 8)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42))]) #('skb', SelectKBest(f_classif, k = 5)),\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bea186d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.9427678922836326\n",
      "Test AUC is:  0.7578762643281067\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eef588",
   "metadata": {},
   "source": [
    "### XGB hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc9249a",
   "metadata": {},
   "source": [
    "##### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d85d889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 30)),\n",
    "#         ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 8)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42))]) #('skb', SelectKBest(f_classif, k = 5)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a12a6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:   58.1s\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=5)]: Done 300 out of 300 | elapsed: 10.7min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.76666095951876\n",
      "{'model__subsample': 0.8, 'model__reg_alpha': 1, 'model__n_estimators': 50, 'model__min_child_weight': 12, 'model__max_depth': 5, 'model__gamma': 0.3, 'model__colsample_bytree': 0.9, 'data_pre_process__num__skb__k': 31, 'data_pre_process__cat__skb__k': 7}\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'data_pre_process__num__skb__k':[i for i in range(10,35)],\n",
    "         'data_pre_process__cat__skb__k':[i for i in range(1,8)],\n",
    "         'model__max_depth':range(3,10,2),\n",
    "         'model__n_estimators': range(50, 1000, 50),\n",
    "         'model__min_child_weight':range(1,6,2),\n",
    "         'model__max_depth':[4,5,6],\n",
    "         'model__min_child_weight':[4,5,6],\n",
    "         'model__min_child_weight':[6,8,10,12],\n",
    "         'model__gamma':[i/10.0 for i in range(0,5)],\n",
    "         'model__subsample':[i/10.0 for i in range(6,10)],\n",
    "         'model__colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "         'model__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8c0e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 31)),\n",
    "#         ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 7)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(subsample= 0.8, reg_alpha= 1, n_estimators = 50, min_child_weight= 12, max_depth= 5, gamma=0.3, colsample_bytree= 0.9))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59ee2644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.8356982078071512\n",
      "Test AUC is:  0.7671660217226716\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217996a",
   "metadata": {},
   "source": [
    "##### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85d0db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9515656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 8)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42))]) #('skb', SelectKBest(f_classif, k = 5)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a12a6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=5)]: Done 300 out of 300 | elapsed: 10.6min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:02:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7583779005685299\n",
      "{'model__subsample': 0.8, 'model__reg_alpha': 100, 'model__n_estimators': 100, 'model__min_child_weight': 8, 'model__max_depth': 9, 'model__gamma': 0.2, 'model__colsample_bytree': 0.7, 'data_pre_process__num__skb__k': 33, 'data_pre_process__cat__skb__k': 5}\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'data_pre_process__num__skb__k':[i for i in range(1,35)],\n",
    "         'data_pre_process__cat__skb__k':[i for i in range(1,8)],\n",
    "         'model__max_depth':range(3,10,2),\n",
    "         'model__n_estimators': range(50, 1000, 50),\n",
    "         'model__min_child_weight':[4,5,6,8,10,12],\n",
    "         'model__gamma':[i/10.0 for i in range(0,5)],\n",
    "         'model__subsample':[i/10.0 for i in range(2,10)],\n",
    "         'model__colsample_bytree':[i/10.0 for i in range(2,10)],\n",
    "         'model__reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8c0e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:04:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 33)),\n",
    "#         ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 5)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(subsample= 0.8, reg_alpha= 100, n_estimators = 100, min_child_weight= 8, max_depth= 9, gamma=0.2, colsample_bytree= 0.7))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59ee2644",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7867068859358495\n",
      "Test AUC is:  0.7650781007297437\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57584f",
   "metadata": {},
   "source": [
    "##### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "23361b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d85d889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 8)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42))]) #('skb', SelectKBest(f_classif, k = 5)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'data_pre_process__num__skb__k':[i for i in range(1,35)],\n",
    "         'data_pre_process__cat__skb__k':[i for i in range(1,8)],\n",
    "         'model__max_depth':range(3,10,2),\n",
    "         'model__n_estimators': range(50, 2000, 50),\n",
    "         'model__min_child_weight':[4,5,6,8,10,12],\n",
    "         'model__gamma':[i/10.0 for i in range(0,5)],\n",
    "         'model__subsample':[i/10.0 for i in range(2,10)],\n",
    "         'model__colsample_bytree':[i/10.0 for i in range(2,10)],\n",
    "         'model__reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 500, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8c0e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:114: UserWarning: Features [0] are constant.\n",
      "  UserWarning)\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:04:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        ('skb', SelectKBest(f_classif, k = 33)),\n",
    "#         ('std', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         ('skb', SelectKBest(chi2, k = 5)),\n",
    "          ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(subsample= 0.8, reg_alpha= 100, n_estimators = 100, min_child_weight= 8, max_depth= 9, gamma=0.2, colsample_bytree= 0.7))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "59ee2644",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.7867068859358495\n",
      "Test AUC is:  0.7650781007297437\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
