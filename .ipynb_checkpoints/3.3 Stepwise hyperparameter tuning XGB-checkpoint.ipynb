{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder, Binarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('Training/X_train.csv')\n",
    "y = pd.read_csv('Training/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf9bb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C6'] = X['C6'].apply(lambda x: 0 if x==False else 1)\n",
    "X['C8'] = X['C8'].apply(lambda x: 0 if x==False else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f7df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,1:]\n",
    "y = y['Dependent_Variable']\n",
    "X_numerical = X.iloc[:,8:]\n",
    "numerical_columns= list(X_numerical.columns)\n",
    "X_categorical = X.iloc[:,:8]\n",
    "categorical_columns= list(X_categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268bd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccef140",
   "metadata": {},
   "source": [
    "##### Find best number of estimators and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb477db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42))]) #('skb', SelectKBest(f_classif, k = 5)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a549f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=5)]: Done 150 out of 150 | elapsed: 11.8min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:06:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7708493360349222\n",
      "{'model__n_estimators': 850, 'model__learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'model__n_estimators': range(50, 1000, 50),\n",
    "         'model__learning_rate':[1e-5, 1e-2, 0.1, 1, 10]\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec51b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, learning_rate=0.01))])\n",
    "history = full_pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbbd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC is:  0.8528385685297251\n",
      "Test AUC is:  0.7772122554207526\n"
     ]
    }
   ],
   "source": [
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76697f",
   "metadata": {},
   "source": [
    "##### Find best max_child and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e089ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=5)]: Done  31 out of  36 | elapsed:  4.7min remaining:   45.1s\n",
      "[Parallel(n_jobs=5)]: Done  36 out of  36 | elapsed:  5.4min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:08:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7715373772098113\n",
      "{'model__min_child_weight': 5, 'model__max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, learning_rate=0.01))])\n",
    "\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'model__max_depth':range(3,10,2),\n",
    "         'model__min_child_weight':range(1,6,2)\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 12, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932276b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:12:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.871917425319153\n",
      "Test AUC is:  0.7790913581856322\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 5,\n",
    "                                                                                              max_depth = 7))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e71785",
   "metadata": {},
   "source": [
    "##### test for min_clild_weight and max child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c6e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=5)]: Done  21 out of  27 | elapsed:  4.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done  24 out of  27 | elapsed:  4.4min remaining:   32.7s\n",
      "[Parallel(n_jobs=5)]: Done  27 out of  27 | elapsed:  4.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done  27 out of  27 | elapsed:  4.7min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:02:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7720589560287695\n",
      "{'model__min_child_weight': 6, 'model__max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             ))])\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'model__max_depth':[6,7,8],\n",
    "         'model__min_child_weight':[4,5,6]\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 9, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256b99a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:12:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.8699815694045232\n",
      "Test AUC is:  0.7792463777313886\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573265ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=5)]: Done  25 out of  30 | elapsed:  4.1min remaining:   49.3s\n",
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed:  4.9min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.772168120235285\n",
      "{'model__gamma': 0.5}\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7))])\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'model__gamma':[i/10.0 for i in range(0,10)]\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40216120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:32:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.8697147983834241\n",
      "Test AUC is:  0.7794108313850472\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "939bf81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=5)]: Done 232 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=5)]: Done 255 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=5)]: Done 300 out of 300 | elapsed: 34.1min finished\n",
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.7765067969659677\n",
      "{'model__subsample': 0.8, 'model__colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5))])\n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "         'model__subsample':[i/10.0 for i in range(1,11)],\n",
    "         'model__colsample_bytree':[i/10.0 for i in range(1,11)]\n",
    "    }\n",
    "xgb_random = RandomizedSearchCV(estimator = full_pipeline, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=10, random_state=42, n_jobs = 5, scoring='roc_auc')\n",
    "xgb_random.fit(X_train,y_train)\n",
    "print(xgb_random.best_score_)\n",
    "print(xgb_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8575d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5,\n",
    "                                                                                             subsample=0.8,\n",
    "                                                                                             colsample_bytree=0.6))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
