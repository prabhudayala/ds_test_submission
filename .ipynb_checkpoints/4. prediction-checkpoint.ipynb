{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa37509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder, Binarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82400160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('Training/X_train.csv')\n",
    "y = pd.read_csv('Training/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['C6'] = X['C6'].apply(lambda x: 0 if x==False else 1)\n",
    "X['C8'] = X['C8'].apply(lambda x: 0 if x==False else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcde59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,1:]\n",
    "y = y['Dependent_Variable']\n",
    "X_numerical = X.iloc[:,8:]\n",
    "numerical_columns= list(X_numerical.columns)\n",
    "X_categorical = X.iloc[:,:8]\n",
    "categorical_columns= list(X_categorical.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dbe8a7",
   "metadata": {},
   "source": [
    "### test_size=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d5a158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:08:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.882145029017232\n",
      "Test AUC is:  0.7728789843787752\n",
      "AUC on whole is:  0.8501415255644422\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5,\n",
    "                                                                                             subsample=0.8,\n",
    "                                                                                             colsample_bytree=0.6))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)\n",
    "\n",
    "y_proba = full_pipeline.predict_proba(X)\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "print(\"AUC on whole is: \",train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bc99e",
   "metadata": {},
   "source": [
    "### test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54b16cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.8765744414742783\n",
      "Test AUC is:  0.7774897807895035\n",
      "AUC on whole is:  0.8571979578415339\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5,\n",
    "                                                                                             subsample=0.8,\n",
    "                                                                                             colsample_bytree=0.6))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)\n",
    "\n",
    "y_proba = full_pipeline.predict_proba(X)\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "print(\"AUC on whole is: \",train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b21b26",
   "metadata": {},
   "source": [
    "### test_size=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606ab997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train AUC is:  0.8692121310968441\n",
      "Test AUC is:  0.7823637071584295\n",
      "AUC on whole is:  0.860676342514916\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5,\n",
    "                                                                                             subsample=0.8,\n",
    "                                                                                             colsample_bytree=0.6))])\n",
    "history = full_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_train_proba = full_pipeline.predict_proba(X_train)\n",
    "y_test_proba = full_pipeline.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba[:,1], pos_label=1)\n",
    "test_auc =  auc(fpr, tpr)\n",
    "print(\"Train AUC is: \",train_auc)\n",
    "print(\"Test AUC is: \",test_auc)\n",
    "\n",
    "y_proba = full_pipeline.predict_proba(X)\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba[:,1], pos_label=1)\n",
    "train_auc =  auc(fpr, tpr)\n",
    "print(\"AUC on whole is: \",train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f42b17",
   "metadata": {},
   "source": [
    "**`The decrease in test size increases the validation accuracy and decreases the train accuracy. So I'll train a model with whole data and use it for test prediction`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83469195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:15:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "AUC on whole is:  0.8634629971389927\n"
     ]
    }
   ],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "          ('poly', PolynomialFeatures(degree=1)),\n",
    "        #('skb', SelectKBest(f_classif, k = 30)),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "         #('skb', SelectKBest(chi2, k = 8)),\n",
    "          #('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "column_combine = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_columns),\n",
    "        (\"cat\", cat_pipeline, categorical_columns),\n",
    "    ])\n",
    "\n",
    "full_pipeline = Pipeline(steps=[('data_pre_process', column_combine), ('model', XGBClassifier(random_state=42,n_estimators=850, \n",
    "                                                                                              learning_rate=0.01,\n",
    "                                                                                             min_child_weight = 6,\n",
    "                                                                                              max_depth = 7,\n",
    "                                                                                              gamma=0.5,\n",
    "                                                                                             subsample=0.8,\n",
    "                                                                                             colsample_bytree=0.6))])\n",
    "history = full_pipeline.fit(X,y)\n",
    "\n",
    "y_proba = full_pipeline.predict_proba(X)\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba[:,1], pos_label=1)\n",
    "all_data_auc =  auc(fpr, tpr)\n",
    "print(\"AUC on whole is: \",all_data_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f7ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Test/X_test.csv')\n",
    "ids =  test_data['Unique_ID']\n",
    "test_data['C6'] = test_data['C6'].apply(lambda x: 0 if x==False else 1)\n",
    "test_data['C8'] = test_data['C8'].apply(lambda x: 0 if x==False else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046b67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow_2_6\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_proba = full_pipeline.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcf6256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Class_1_Probability'] = y_proba[:,1]\n",
    "test_data = test_data[['Unique_ID','Class_1_Probability']]\n",
    "test_data.to_csv(r'Test/final_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d3c6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Class_1_Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candidate_1602</td>\n",
       "      <td>0.369020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candidate_29650</td>\n",
       "      <td>0.321292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candidate_31061</td>\n",
       "      <td>0.424712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candidate_5768</td>\n",
       "      <td>0.188813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candidate_27059</td>\n",
       "      <td>0.454346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>Candidate_7453</td>\n",
       "      <td>0.581452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11013</th>\n",
       "      <td>Candidate_38211</td>\n",
       "      <td>0.115369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014</th>\n",
       "      <td>Candidate_25020</td>\n",
       "      <td>0.393083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11015</th>\n",
       "      <td>Candidate_44501</td>\n",
       "      <td>0.136980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>Candidate_49327</td>\n",
       "      <td>0.181762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11017 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unique_ID  Class_1_Probability\n",
       "0       Candidate_1602             0.369020\n",
       "1      Candidate_29650             0.321292\n",
       "2      Candidate_31061             0.424712\n",
       "3       Candidate_5768             0.188813\n",
       "4      Candidate_27059             0.454346\n",
       "...                ...                  ...\n",
       "11012   Candidate_7453             0.581452\n",
       "11013  Candidate_38211             0.115369\n",
       "11014  Candidate_25020             0.393083\n",
       "11015  Candidate_44501             0.136980\n",
       "11016  Candidate_49327             0.181762\n",
       "\n",
       "[11017 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5693a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
